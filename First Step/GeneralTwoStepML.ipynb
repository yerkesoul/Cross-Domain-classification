{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cd81b1",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "c0cd22f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11176]"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "import subprocess as sp\n",
    "import os\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "3818e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "with open('test_dataset.json', 'r') as fp:\n",
    "    test_dataset = json.load(fp)\n",
    "with open('train_dataset.json', 'r') as fp:\n",
    "    train_dataset = json.load(fp)\n",
    "f = open('/data/data_codebook.json')\n",
    "data_codebook = json.load(f)\n",
    "super_set={}\n",
    "for s in data_codebook:\n",
    "    if s[2]!=\"domain_name\":\n",
    "        if s[2] not in super_set:\n",
    "            super_set[s[2]]=[]\n",
    "        if s[5] not in super_set[s[2]]:\n",
    "            super_set[s[2]].append(s[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "ce87a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    \n",
    "    html=re.compile(r'<.*?>') \n",
    "    \n",
    "    text = html.sub(r'',text) #Removing html tags\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    \n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "29dadbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dataset_counts(dataset):\n",
    "    count_dataset = {\n",
    "        \"general\": {},\n",
    "        \"detailed\": {}\n",
    "    }\n",
    "    \n",
    "    for s in dataset:\n",
    "        detailed_label = s['detailed_label']\n",
    "        general_label = s[\"general_label\"]\n",
    "        \n",
    "        if detailed_label not in count_dataset[\"detailed\"]:\n",
    "            count_dataset[\"detailed\"][detailed_label] = 0\n",
    "        count_dataset[\"detailed\"][detailed_label] += 1\n",
    "\n",
    "        if general_label not in count_dataset[\"general\"]:\n",
    "            count_dataset[\"general\"][general_label] = 0\n",
    "        count_dataset[\"general\"][general_label] += 1\n",
    "    \n",
    "    return count_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "0c5e4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_dataset(balanced_dataset, total_limit, desired_percentage):\n",
    "    used_sentences = set()\n",
    "    dataset = []\n",
    "    super_category_counts = {super_category: 0 for super_category in super_set.keys()}\n",
    "    category_counts = {category: 0 for category in balanced_dataset.keys()}\n",
    "\n",
    "    # Calculate the limit for 'Welfare and Quality of Life' based on desired_percentage\n",
    "    welfare_category = 'Welfare and Quality of Life'\n",
    "    welfare_desired_limit = int(total_limit * desired_percentage)\n",
    "    # Calculate the total number of sentences in the 'Welfare and Quality of Life' super-category\n",
    "    welfare_super_category_sentences = sum(len(balanced_dataset[category]) for category in super_set[welfare_category] if category in balanced_dataset)\n",
    "    # Calculate the limits for categories within the 'Welfare and Quality of Life' super-category\n",
    "    welfare_category_limits = {}\n",
    "    for category in super_set[welfare_category]:\n",
    "        if category in balanced_dataset:\n",
    "            category_percentage = len(balanced_dataset[category]) / welfare_super_category_sentences\n",
    "            welfare_category_limits[category] = int(welfare_desired_limit * category_percentage)\n",
    "    \n",
    "    # Process 'Welfare and Quality of Life' category first\n",
    "    welfare_categories = super_set[welfare_category]\n",
    "    #print(welfare_super_category_sentences)\n",
    "    for category in welfare_categories:\n",
    "        if category not in balanced_dataset:\n",
    "            continue\n",
    "        \n",
    "        sentences = balanced_dataset[category]\n",
    "        category_limit = welfare_category_limits[category]\n",
    "        for sentence in sentences:\n",
    "            if len(dataset) >= welfare_desired_limit or len(dataset) >= total_limit:\n",
    "                break\n",
    "\n",
    "            if sentence not in used_sentences and super_category_counts[welfare_category] < welfare_desired_limit and category_limit > 0:\n",
    "                per_line_dict = {\n",
    "                    \"sentence\": sentence,\n",
    "                    \"detailed_label\": category,\n",
    "                    \"general_label\": welfare_category\n",
    "                }\n",
    "                dataset.append(per_line_dict)\n",
    "                used_sentences.add(sentence)\n",
    "                super_category_counts[welfare_category] += 1\n",
    "                category_limit -= 1    \n",
    "                \n",
    "    def get_max_value_key_and_value(dictionary):\n",
    "        max_key = max(dictionary, key=dictionary.get)\n",
    "        max_value = dictionary[max_key]\n",
    "        return max_key, max_value\n",
    "    \n",
    "    if len(dataset)<welfare_desired_limit:\n",
    "        w_max_key, w_max_value=get_max_value_key_and_value(welfare_category_limits)\n",
    "        missing_part=welfare_desired_limit-len(dataset)\n",
    "        for sentence in balanced_dataset[w_max_key]:\n",
    "            if  missing_part>0:\n",
    "                if sentence not in used_sentences and super_category_counts[welfare_category] < welfare_desired_limit:\n",
    "                    per_line_dict = {\n",
    "                        \"sentence\": sentence,\n",
    "                        \"detailed_label\": w_max_key,\n",
    "                        \"general_label\": welfare_category\n",
    "                    }\n",
    "                    dataset.append(per_line_dict)\n",
    "                    used_sentences.add(sentence)\n",
    "                    super_category_counts[welfare_category] += 1\n",
    "                    missing_part-= 1\n",
    "    # Calculate the limit for other super categories\n",
    "    other_desired_limit = total_limit - welfare_desired_limit\n",
    "    \n",
    "    def get_length(d):\n",
    "        ''' Return length of all dict values'''\n",
    "        return sum(len(v) for k, v in d.items())\n",
    "    \n",
    "    total_length=get_length(balanced_dataset)\n",
    "    non_welfare_sentences_count=total_length-welfare_super_category_sentences\n",
    "    non_welfare_category_limits = {}\n",
    "    for super_category, categories in super_set.items(): \n",
    "        if super_category==welfare_category:\n",
    "            continue\n",
    "        for category in categories:\n",
    "            if category not in balanced_dataset:\n",
    "                continue\n",
    "            non_welfare_category_percentage = len(balanced_dataset[category]) / non_welfare_sentences_count\n",
    "            non_welfare_category_limits[category] = int(other_desired_limit * non_welfare_category_percentage)\n",
    "    for super_category, categories in super_set.items(): \n",
    "        if super_category==welfare_category:\n",
    "            continue\n",
    "            \n",
    "        for category in categories:\n",
    "            if category not in balanced_dataset:\n",
    "                continue\n",
    "            sentences = balanced_dataset[category]\n",
    "            category_limit = non_welfare_category_limits[category]\n",
    "            for sentence in sentences:\n",
    "                \n",
    "                if sentence not in used_sentences and super_category_counts[super_category] < other_desired_limit and category_limit > 0:\n",
    "                    \n",
    "                    per_line_dict = {\n",
    "                        \"sentence\": sentence,\n",
    "                        \"detailed_label\": category,\n",
    "                        \"general_label\": super_category\n",
    "                    }\n",
    "                    dataset.append(per_line_dict)\n",
    "                    used_sentences.add(sentence)\n",
    "                    super_category_counts[super_category] += 1\n",
    "                    category_limit -= 1\n",
    "    \n",
    "    if len(dataset)< total_limit:\n",
    "        wn_max_key, wn_max_value=get_max_value_key_and_value(non_welfare_category_limits)\n",
    "        for super_category, categories in super_set.items(): \n",
    "            if wn_max_key in categories:\n",
    "                additional_super_category=super_category\n",
    "        missing_part=total_limit-len(dataset)\n",
    "        for sentence in balanced_dataset[wn_max_key]:\n",
    "            if  missing_part>0:\n",
    "                if sentence not in used_sentences:\n",
    "                    per_line_dict = {\n",
    "                        \"sentence\": sentence,\n",
    "                        \"detailed_label\": wn_max_key,\n",
    "                        \"general_label\": additional_super_category\n",
    "                    }\n",
    "                    dataset.append(per_line_dict)\n",
    "                    used_sentences.add(sentence)\n",
    "                    super_category_counts[additional_super_category] += 1\n",
    "                    missing_part-= 1\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "a6ebbc21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in the dataset: 100000\n",
      "{'Welfare and Quality of Life': 10000, 'External Relations': 10507, 'Freedom and Democracy': 8570, 'Political System': 13519, 'Economy': 34878, 'Fabric of Society': 15369, 'Social Groups': 7157}\n",
      "Detailed categories inside general category:\n",
      "---------------------------\n",
      "General label: NA\n",
      "---------------------------\n",
      "General label: External Relations\n",
      "Foreign Special Relationships: Positive : 385\n",
      "Foreign Special Relationships: Negative : 136\n",
      "Anti-Imperialism : 62\n",
      "Military: Positive : 2160\n",
      "Military: Negative : 518\n",
      "Peace : 531\n",
      "Internationalism: Positive : 3567\n",
      "European Community/Union: Positive : 2164\n",
      "Internationalism: Negative : 260\n",
      "European Community/Union: Negative : 595\n",
      "Russia/USSR/CIS: Positive : 3\n",
      "Western States: Positive : 3\n",
      "SFR Yugoslavia: Positive : 7\n",
      "Independence: Positive : 5\n",
      "Anti-Imperialism: Foreign Financial Influence : 111\n",
      "---------------------------\n",
      "General label: Freedom and Democracy\n",
      "Freedom and Human Rights : 1850\n",
      "Democracy : 1468\n",
      "Constitutionalism: Positive : 438\n",
      "Constitutionalism: Negative : 348\n",
      "Transition to Democracy : 1\n",
      "Freedom : 658\n",
      "Human Rights : 1147\n",
      "Democracy General: Positive : 2244\n",
      "Democracy General: Negative : 17\n",
      "Representative Democracy: Positive : 91\n",
      "Direct Democracy: Positive : 308\n",
      "---------------------------\n",
      "General label: Political System\n",
      "Decentralization : 3141\n",
      "Centralisation : 158\n",
      "Governmental and Administrative Efficiency : 5976\n",
      "Political Corruption : 1313\n",
      "Political Authority : 1786\n",
      "Public Situation: Negative : 1\n",
      "Communist: Negative : 6\n",
      "Rehabilitation and Compensation: Positive : 1\n",
      "Political Coalitions: Positive : 2\n",
      "Political Authority: Party Competence : 992\n",
      "Political Authority: Personal Competence : 53\n",
      "Political Authority: Strong government : 56\n",
      "Transition: Pre-Democratic Elites: Negative : 26\n",
      "Transition: Rehabilitation and Compensation : 8\n",
      "---------------------------\n",
      "General label: Economy\n",
      "Free Market Economy : 1394\n",
      "Incentives: Positive : 3921\n",
      "Market Regulation : 4151\n",
      "Economic Planning : 895\n",
      "Corporatism/Mixed Economy : 331\n",
      "Protectionism: Positive : 498\n",
      "Protectionism: Negative : 323\n",
      "Economic Goals : 2360\n",
      "Keynesian Demand Management : 314\n",
      "Economic Growth: Positive : 2916\n",
      "Technology and Infrastructure: Positive : 11050\n",
      "Controlled Economy : 416\n",
      "Nationalisation : 366\n",
      "Economic Orthodoxy : 1834\n",
      "Marxist Analysis : 164\n",
      "Anti-Growth Economy: Positive : 1351\n",
      "Control of Economy: Negative : 17\n",
      "Property-Restitution: Positive : 3\n",
      "Mixed Economy: Positive : 3\n",
      "Publicly-Owned Industry: Positive : 1\n",
      "Privatisation: Negative : 2\n",
      "Sustainability: Positive : 2568\n",
      "---------------------------\n",
      "General label: Welfare and Quality of Life\n",
      "Environmental Protection : 1438\n",
      "Culture: Positive : 1176\n",
      "Equality: Positive : 2176\n",
      "Welfare State Expansion : 3425\n",
      "Welfare State Limitation : 183\n",
      "Education Expansion : 1576\n",
      "Education Limitation : 17\n",
      "Private-Public Mix in Culture: Positive : 1\n",
      "Private-Public Mix in Welfare: Positive : 7\n",
      "Private-Public Mix in Education: Positive : 1\n",
      "---------------------------\n",
      "General label: Fabric of Society\n",
      "National Way of Life: Positive : 933\n",
      "National Way of Life: Negative : 90\n",
      "Traditional Morality: Positive : 2166\n",
      "Traditional Morality: Negative : 839\n",
      "Law and Order: Positive : 5322\n",
      "Civic Mindedness: Positive : 901\n",
      "Multiculturalism: Positive : 687\n",
      "Multiculturalism: Negative : 325\n",
      "National Security: Positive : 24\n",
      "Cyprus Issue : 66\n",
      "General Crisis : 8\n",
      "National Way of Life General: Positive : 1248\n",
      "National Way of Life: Immigration: Negative : 332\n",
      "National Way of Life General: Negative : 151\n",
      "National Way of Life: Immigration: Positive : 299\n",
      "Law and Order: Negative : 262\n",
      "Civic Mindedness General: Positive : 848\n",
      "Civic Mindedness: Bottom-Up Activism : 85\n",
      "Multiculturalism General: Positive : 456\n",
      "Multiculturalism: Immigrants Diversity : 89\n",
      "Multiculturalism: Indigenous rights: Positive : 71\n",
      "Multiculturalism General: Negative : 105\n",
      "Multiculturalism: Immigrants Assimilation : 57\n",
      "Multiculturalism: Indigenous rights: Negative : 5\n",
      "---------------------------\n",
      "General label: Social Groups\n",
      "Agriculture and Farmers: Positive : 3926\n",
      "Middle Class and Professional Groups : 685\n",
      "Underprivileged Minority Groups : 1116\n",
      "Non-economic Demographic Groups : 1365\n",
      "Minorities Abroad: Positive : 10\n",
      "War Participants: Positive : 20\n",
      "Refugees: Positive : 10\n",
      "Agriculture and Farmers: Negative : 25\n"
     ]
    }
   ],
   "source": [
    "\"Experiment 1\"\n",
    "total_limit = 100000\n",
    "desired_percentage = 0.10\n",
    "dataset = create_custom_dataset(train_dataset, total_limit, desired_percentage)\n",
    "\n",
    "print(\"Total sentences in the dataset:\", len(dataset))\n",
    "\n",
    "count_dataset = calculate_dataset_counts(dataset)\n",
    "print(count_dataset[\"general\"])\n",
    "print(\"Detailed categories inside general category:\")\n",
    "for k,v in super_set.items():\n",
    "    print(\"---------------------------\")\n",
    "    print(\"General label:\",k)\n",
    "    for c in v:\n",
    "        if c in count_dataset[\"detailed\"]:\n",
    "            print(c,\":\",count_dataset[\"detailed\"][c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "e55313a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classification(dataframe, test_data):\n",
    "    dataframe = pd.DataFrame(data=(dataframe))\n",
    "    dataframe=shuffle(dataframe).dropna()\n",
    "    train_sentences=list(dataframe[\"sentence\"])\n",
    "    train_labels=[]\n",
    "    for s in list(dataframe['general_label']):\n",
    "        if s=='Welfare and Quality of Life':\n",
    "            number=1\n",
    "        else:\n",
    "            number=0\n",
    "        train_labels.append(number)\n",
    "    len(train_sentences)==len(train_labels)\n",
    "\n",
    "    train_sentences_cleaned=[]\n",
    "    for s in list(dataframe[\"sentence\"]):\n",
    "        cleaned=clean_text(s)\n",
    "        train_sentences_cleaned.append(cleaned)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_sentences_cleaned, train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "    # Train the models\n",
    "    logreg_model = LogisticRegression(max_iter=1000)\n",
    "    gnb_model = GaussianNB()\n",
    "    svc_model = LinearSVC()\n",
    "\n",
    "    logreg_model.fit(X_train_vectorized, y_train)\n",
    "    gnb_model.fit(X_train_vectorized.toarray(), y_train)\n",
    "    svc_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    logreg_pred = logreg_model.predict(X_test_vectorized)\n",
    "    gnb_pred = gnb_model.predict(X_test_vectorized.toarray())\n",
    "    svc_pred = svc_model.predict(X_test_vectorized)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    logreg_accuracy = accuracy_score(y_test, logreg_pred)\n",
    "    gnb_accuracy = accuracy_score(y_test, gnb_pred)\n",
    "    svc_accuracy = accuracy_score(y_test, svc_pred)\n",
    "\n",
    "    # Evaluate F1-score\n",
    "    logreg_f1_score = f1_score(y_test, logreg_pred, average='weighted')\n",
    "    gnb_f1_score = f1_score(y_test, gnb_pred, average='weighted')\n",
    "    svc_f1_score = f1_score(y_test, svc_pred, average='weighted')\n",
    "\n",
    "    # Evaluate balanced accuracy\n",
    "    logreg_balanced_accuracy = balanced_accuracy_score(y_test, logreg_pred)\n",
    "    gnb_balanced_accuracy = balanced_accuracy_score(y_test, gnb_pred)\n",
    "    svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_pred)\n",
    "\n",
    "    # Evaluate ROC-AUC score\n",
    "    logreg_roc_auc_score = roc_auc_score(y_test, logreg_model.predict_proba(X_test_vectorized)[:, 1], multi_class='ovr')\n",
    "    gnb_roc_auc_score = roc_auc_score(y_test, gnb_model.predict_proba(X_test_vectorized.toarray())[:, 1], multi_class='ovr')\n",
    "    svc_roc_auc_score = roc_auc_score(y_test, svc_model.decision_function(X_test_vectorized), multi_class='ovr')\n",
    "\n",
    "    # Print evaluation metrics for Logistic Regression\n",
    "    print(\"Logistic Regression Accuracy:\", logreg_accuracy)\n",
    "    print(\"Logistic Regression F1-Score:\", logreg_f1_score)\n",
    "    print(\"Logistic Regression Balanced Accuracy:\", logreg_balanced_accuracy)\n",
    "    print(\"Logistic Regression ROC-AUC Score:\", logreg_roc_auc_score)\n",
    "    print()\n",
    "\n",
    "    # Print evaluation metrics for Gaussian Naive Bayes\n",
    "    print(\"Gaussian Naive Bayes Accuracy:\", gnb_accuracy)\n",
    "    print(\"Gaussian Naive Bayes F1-Score:\", gnb_f1_score)\n",
    "    print(\"Gaussian Naive Bayes Balanced Accuracy:\", gnb_balanced_accuracy)\n",
    "    print(\"Gaussian Naive Bayes ROC-AUC Score:\", gnb_roc_auc_score)\n",
    "    print()\n",
    "\n",
    "    # Print evaluation metrics for Linear SVC\n",
    "    print(\"Linear SVC Accuracy:\", svc_accuracy)\n",
    "    print(\"Linear SVC F1-Score:\", svc_f1_score)\n",
    "    print(\"Linear SVC Balanced Accuracy:\", svc_balanced_accuracy)\n",
    "    print(\"Linear SVC ROC-AUC Score:\", svc_roc_auc_score)\n",
    "    print()\n",
    "\n",
    "    # Print classification report and confusion matrix for Linear SVC\n",
    "    print(\"Linear SVC Classification Report:\")\n",
    "    print(classification_report(y_test, svc_pred))\n",
    "\n",
    "    print(\"Linear SVC Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, svc_pred))\n",
    "\n",
    "    print(\"-----Now test the model on another unseen test sets--------\")\n",
    "    test_dataframe=[]\n",
    "    for k,v in test_data.items():\n",
    "        for key, value in super_set.items():\n",
    "            if k in value:\n",
    "                super_label = key\n",
    "        for s in v:\n",
    "                per_line_dict = {}\n",
    "                per_line_dict[\"sentence\"] = s\n",
    "                per_line_dict[\"detailed_label\"] = k\n",
    "                per_line_dict[\"general_label\"] = super_label\n",
    "                test_dataframe.append(per_line_dict)\n",
    "                \n",
    "    test_dataframe = pd.DataFrame(data=(test_dataframe))\n",
    "    test_dataframe=shuffle(test_dataframe).dropna()\n",
    "    test_sentences_cleaned=[]\n",
    "    for s in list(test_dataframe[\"sentence\"]):\n",
    "        cleaned=clean_text(s)\n",
    "        test_sentences_cleaned.append(cleaned)\n",
    "        \n",
    "    y_test=[]\n",
    "    for s in list(test_dataframe['general_label']):\n",
    "        if s=='Welfare and Quality of Life':\n",
    "            number=1\n",
    "        else:\n",
    "            number=0\n",
    "        y_test.append(number)\n",
    "\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    X_vectorized = vectorizer.transform(test_sentences_cleaned)\n",
    "    svc_pred = svc_model.predict(X_vectorized)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    svc_accuracy = accuracy_score(y_test, svc_pred)\n",
    "\n",
    "    # Evaluate F1-score\n",
    "    svc_f1_score = f1_score(y_test, svc_pred, average='weighted')\n",
    "\n",
    "    # Evaluate balanced accuracy\n",
    "    svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_pred)\n",
    "\n",
    "    # Evaluate ROC-AUC score\n",
    "    svc_roc_auc_score = roc_auc_score(y_test, svc_model.decision_function(X_vectorized), multi_class='ovr')\n",
    "\n",
    "    # Print evaluation metrics for Linear SVC on the unseen test set\n",
    "    print(\"Linear SVC Accuracy on Unseen Test Set:\", round(svc_accuracy,2))\n",
    "    print(\"Linear SVC F1-Score on Unseen Test Set:\", round(svc_f1_score,2))\n",
    "    print(\"Linear SVC Balanced Accuracy on Unseen Test Set:\", round(svc_balanced_accuracy,2))\n",
    "    print(\"Linear SVC ROC-AUC Score on Unseen Test Set:\", round(svc_roc_auc_score,2))\n",
    "    print()\n",
    "\n",
    "    # Print classification report and confusion matrix for Linear SVC on the unseen test set\n",
    "    print(\"Linear SVC Classification Report on Unseen Test Set:\")\n",
    "    print(classification_report(y_test, svc_pred))\n",
    "\n",
    "    print(\"Linear SVC Confusion Matrix on Unseen Test Set:\")\n",
    "    print(confusion_matrix(y_test, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "4f23757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9059\n",
      "Logistic Regression F1-Score: 0.8775483904292336\n",
      "Logistic Regression Balanced Accuracy: 0.5603305378535451\n",
      "Logistic Regression ROC-AUC Score: 0.8295013984873945\n",
      "\n",
      "Gaussian Naive Bayes Accuracy: 0.4623333333333333\n",
      "Gaussian Naive Bayes F1-Score: 0.5553252495308383\n",
      "Gaussian Naive Bayes Balanced Accuracy: 0.5736816046493882\n",
      "Gaussian Naive Bayes ROC-AUC Score: 0.5736816046493882\n",
      "\n",
      "Linear SVC Accuracy: 0.9039666666666667\n",
      "Linear SVC F1-Score: 0.8833842684687337\n",
      "Linear SVC Balanced Accuracy: 0.5885425384675644\n",
      "Linear SVC ROC-AUC Score: 0.802865525156919\n",
      "\n",
      "Linear SVC Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95     27049\n",
      "           1       0.53      0.20      0.29      2951\n",
      "\n",
      "    accuracy                           0.90     30000\n",
      "   macro avg       0.73      0.59      0.62     30000\n",
      "weighted avg       0.88      0.90      0.88     30000\n",
      "\n",
      "Linear SVC Confusion Matrix:\n",
      "[[26541   508]\n",
      " [ 2373   578]]\n",
      "-----Now test the model on another unseen test sets--------\n",
      "Linear SVC Accuracy on Unseen Test Set: 0.7\n",
      "Linear SVC F1-Score on Unseen Test Set: 0.63\n",
      "Linear SVC Balanced Accuracy on Unseen Test Set: 0.58\n",
      "Linear SVC ROC-AUC Score on Unseen Test Set: 0.79\n",
      "\n",
      "Linear SVC Classification Report on Unseen Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.98      0.81     55471\n",
      "           1       0.84      0.18      0.30     29389\n",
      "\n",
      "    accuracy                           0.70     84860\n",
      "   macro avg       0.76      0.58      0.55     84860\n",
      "weighted avg       0.74      0.70      0.63     84860\n",
      "\n",
      "Linear SVC Confusion Matrix on Unseen Test Set:\n",
      "[[54438  1033]\n",
      " [24119  5270]]\n"
     ]
    }
   ],
   "source": [
    "text_classification(dataset,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfe740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e520706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
