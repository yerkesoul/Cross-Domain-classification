{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cd81b1",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f916d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11170]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "import subprocess as sp\n",
    "import os\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c20c53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "with open('test_dataset.json', 'r') as fp:\n",
    "    test_dataset = json.load(fp)\n",
    "with open('train_dataset.json', 'r') as fp:\n",
    "    train_dataset = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "069521a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/data/data_codebook.json')\n",
    "data_codebook = json.load(f)\n",
    "super_set={}\n",
    "for s in data_codebook:\n",
    "    if s[2]!=\"domain_name\":\n",
    "        if s[2] not in super_set:\n",
    "            super_set[s[2]]=[]\n",
    "        if s[5] not in super_set[s[2]]:\n",
    "            super_set[s[2]].append(s[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1536de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    \n",
    "    html=re.compile(r'<.*?>') \n",
    "    \n",
    "    text = html.sub(r'',text) #Removing html tags\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    \n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c14aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_welfare_categories(dataset):\n",
    "    welfare_categories = {}\n",
    "    for key, value in super_set.items():\n",
    "        if key != 'Welfare and Quality of Life':\n",
    "            continue\n",
    "        for category in value:\n",
    "            welfare_categories[category] =dataset[category]\n",
    "    \n",
    "    return welfare_categories\n",
    "\n",
    "welfare_categories_train=extract_welfare_categories(train_dataset)\n",
    "welfare_categories_test=extract_welfare_categories(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac8ed99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dataset_counts(dataset):\n",
    "    count_dataset = {\n",
    "        \"general\": {},\n",
    "        \"detailed\": {}\n",
    "    }\n",
    "    \n",
    "    for s in dataset:\n",
    "        detailed_label = s['detailed_label']\n",
    "        general_label = s[\"general_label\"]\n",
    "        \n",
    "        if detailed_label not in count_dataset[\"detailed\"]:\n",
    "            count_dataset[\"detailed\"][detailed_label] = 0\n",
    "        count_dataset[\"detailed\"][detailed_label] += 1\n",
    "\n",
    "        if general_label not in count_dataset[\"general\"]:\n",
    "            count_dataset[\"general\"][general_label] = 0\n",
    "        count_dataset[\"general\"][general_label] += 1\n",
    "    \n",
    "    return count_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8332aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_dataset(balanced_dataset, total_limit, desired_percentage):\n",
    "    used_sentences = set()\n",
    "    dataset = []\n",
    "    welfare_count = 0\n",
    "    welfare_limit = int(total_limit * desired_percentage)\n",
    "    category_limits = {category: 100 for category in balanced_dataset.keys()}\n",
    "\n",
    "    # Calculate the total number of sentences in the dataset\n",
    "    total_sentences = sum(len(v) for v in balanced_dataset.values())\n",
    "\n",
    "    # First, add sentences from 'Environmental Protection'\n",
    "    for sentence in balanced_dataset['Environmental Protection']:\n",
    "        if len(dataset) >= welfare_limit:\n",
    "            break\n",
    "\n",
    "        if sentence not in used_sentences:\n",
    "            per_line_dict = {\n",
    "                \"sentence\": sentence,\n",
    "                \"detailed_label\": 'Environmental Protection',\n",
    "                \"general_label\": 'Welfare and Quality of Life'\n",
    "            }\n",
    "            dataset.append(per_line_dict)\n",
    "            used_sentences.add(sentence)\n",
    "            welfare_count += 1\n",
    "\n",
    "    # Then, add sentences from other categories\n",
    "    while len(dataset) < total_limit:\n",
    "        for category, sentences in balanced_dataset.items():\n",
    "            if len(dataset) >= total_limit:\n",
    "                break\n",
    "            if category != 'Environmental Protection':\n",
    "                for key, value in super_set.items():\n",
    "                    if category in value:\n",
    "                        super_label = key\n",
    "\n",
    "                category_limit = min(len(sentences), category_limits[category])\n",
    "\n",
    "                limit = min(100, category_limit)\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    if limit == 0 or len(dataset) >= total_limit:\n",
    "                        break\n",
    "\n",
    "                    if sentence not in used_sentences:\n",
    "                        per_line_dict = {\n",
    "                            \"sentence\": sentence,\n",
    "                            \"detailed_label\": category,\n",
    "                            \"general_label\": super_label\n",
    "                        }\n",
    "                        dataset.append(per_line_dict)\n",
    "                        used_sentences.add(sentence)\n",
    "                        limit -= 1\n",
    "                        category_limits[category] += 100\n",
    "\n",
    "    return dataset, welfare_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d91ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in the dataset: 56000\n",
      "Sentences with 'Welfare and Quality of Life' label: 22400\n",
      "{'Welfare and Quality of Life': 56000}\n",
      "56000\n",
      "11\n",
      "{'Environmental Protection': 22400, 'Culture: Positive': 6900, 'Equality: Positive': 6894, 'Welfare State Expansion': 6800, 'Welfare State Limitation': 5380, 'Education Expansion': 6800, 'Education Limitation': 515, 'Private-Public Mix in Culture: Positive': 36, 'Private-Public Mix in Social Justice: Positive': 1, 'Private-Public Mix in Welfare: Positive': 227, 'Private-Public Mix in Education: Positive': 47}\n"
     ]
    }
   ],
   "source": [
    "\"Experiment 1\"\n",
    "total_limit = 56000\n",
    "desired_percentage = 0.4\n",
    "\n",
    "dataset, welfare_count = create_custom_dataset(welfare_categories_train, total_limit, desired_percentage)\n",
    "\n",
    "print(\"Total sentences in the dataset:\", len(dataset))\n",
    "print(\"Sentences with 'Welfare and Quality of Life' label:\", welfare_count)\n",
    "\n",
    "# Usage example:\n",
    "# Assuming you have the 'dataset' variable containing the dataset obtained from the create_custom_dataset function\n",
    "# Replace this with the actual dataset you want to count.\n",
    "count_dataset = calculate_dataset_counts(dataset)\n",
    "print(count_dataset[\"general\"])\n",
    "print(sum(count_dataset[\"general\"].values()))\n",
    "print(len(count_dataset[\"detailed\"].keys()))\n",
    "print(count_dataset[\"detailed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eabbd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classification(dataframe, test_data):\n",
    "    dataframe = pd.DataFrame(data=(dataframe))\n",
    "    dataframe=shuffle(dataframe).dropna()\n",
    "    train_sentences=list(dataframe[\"sentence\"])\n",
    "    train_labels=[]\n",
    "    for s in list(dataframe[\"detailed_label\"]):\n",
    "        if s=='Environmental Protection':\n",
    "            number=1\n",
    "        else:\n",
    "            number=0\n",
    "        train_labels.append(number)\n",
    "    len(train_sentences)==len(train_labels)\n",
    "\n",
    "    train_sentences_cleaned=[]\n",
    "    for s in list(dataframe[\"sentence\"]):\n",
    "        cleaned=clean_text(s)\n",
    "        train_sentences_cleaned.append(cleaned)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_sentences_cleaned, train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "    # Train the models\n",
    "    logreg_model = LogisticRegression(max_iter=1000)\n",
    "    gnb_model = GaussianNB()\n",
    "    svc_model = LinearSVC()\n",
    "\n",
    "    logreg_model.fit(X_train_vectorized, y_train)\n",
    "    gnb_model.fit(X_train_vectorized.toarray(), y_train)\n",
    "    svc_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    logreg_pred = logreg_model.predict(X_test_vectorized)\n",
    "    gnb_pred = gnb_model.predict(X_test_vectorized.toarray())\n",
    "    svc_pred = svc_model.predict(X_test_vectorized)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    logreg_accuracy = accuracy_score(y_test, logreg_pred)\n",
    "    gnb_accuracy = accuracy_score(y_test, gnb_pred)\n",
    "    svc_accuracy = accuracy_score(y_test, svc_pred)\n",
    "\n",
    "    # Evaluate F1-score\n",
    "    logreg_f1_score = f1_score(y_test, logreg_pred, average='weighted')\n",
    "    gnb_f1_score = f1_score(y_test, gnb_pred, average='weighted')\n",
    "    svc_f1_score = f1_score(y_test, svc_pred, average='weighted')\n",
    "\n",
    "    # Evaluate balanced accuracy\n",
    "    logreg_balanced_accuracy = balanced_accuracy_score(y_test, logreg_pred)\n",
    "    gnb_balanced_accuracy = balanced_accuracy_score(y_test, gnb_pred)\n",
    "    svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_pred)\n",
    "\n",
    "    # Evaluate ROC-AUC score\n",
    "    logreg_roc_auc_score = roc_auc_score(y_test, logreg_model.predict_proba(X_test_vectorized)[:, 1], multi_class='ovr')\n",
    "    gnb_roc_auc_score = roc_auc_score(y_test, gnb_model.predict_proba(X_test_vectorized.toarray())[:, 1], multi_class='ovr')\n",
    "    svc_roc_auc_score = roc_auc_score(y_test, svc_model.decision_function(X_test_vectorized), multi_class='ovr')\n",
    "\n",
    "    # Print evaluation metrics for Logistic Regression\n",
    "    print(\"Logistic Regression Accuracy:\", logreg_accuracy)\n",
    "    print(\"Logistic Regression F1-Score:\", logreg_f1_score)\n",
    "    print(\"Logistic Regression Balanced Accuracy:\", logreg_balanced_accuracy)\n",
    "    print(\"Logistic Regression ROC-AUC Score:\", logreg_roc_auc_score)\n",
    "    print()\n",
    "\n",
    "    # Print evaluation metrics for Gaussian Naive Bayes\n",
    "    print(\"Gaussian Naive Bayes Accuracy:\", gnb_accuracy)\n",
    "    print(\"Gaussian Naive Bayes F1-Score:\", gnb_f1_score)\n",
    "    print(\"Gaussian Naive Bayes Balanced Accuracy:\", gnb_balanced_accuracy)\n",
    "    print(\"Gaussian Naive Bayes ROC-AUC Score:\", gnb_roc_auc_score)\n",
    "    print()\n",
    "\n",
    "    # Print evaluation metrics for Linear SVC\n",
    "    print(\"Linear SVC Accuracy:\", svc_accuracy)\n",
    "    print(\"Linear SVC F1-Score:\", svc_f1_score)\n",
    "    print(\"Linear SVC Balanced Accuracy:\", svc_balanced_accuracy)\n",
    "    print(\"Linear SVC ROC-AUC Score:\", svc_roc_auc_score)\n",
    "    print()\n",
    "\n",
    "    # Print classification report and confusion matrix for Linear SVC\n",
    "    print(\"Linear SVC Classification Report:\")\n",
    "    print(classification_report(y_test, svc_pred))\n",
    "\n",
    "    print(\"Linear SVC Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, svc_pred))\n",
    "\n",
    "    print(\"-----Now test the model on another unseen test sets--------\")\n",
    "    test_dataframe=[]\n",
    "    for k,v in test_data.items():\n",
    "        for key, value in super_set.items():\n",
    "            if k in value:\n",
    "                super_label = key\n",
    "        for s in v:\n",
    "                per_line_dict = {}\n",
    "                per_line_dict[\"sentence\"] = s\n",
    "                per_line_dict[\"detailed_label\"] = k\n",
    "                per_line_dict[\"general_label\"] = super_label\n",
    "                test_dataframe.append(per_line_dict)\n",
    "                \n",
    "    test_dataframe = pd.DataFrame(data=(test_dataframe))\n",
    "    test_dataframe=shuffle(test_dataframe).dropna()\n",
    "    test_sentences_cleaned=[]\n",
    "    for s in list(test_dataframe[\"sentence\"]):\n",
    "        cleaned=clean_text(s)\n",
    "        test_sentences_cleaned.append(cleaned)\n",
    "        \n",
    "    y_test=[]\n",
    "    for s in list(test_dataframe[\"detailed_label\"]):\n",
    "        if s=='Environmental Protection':\n",
    "            number=1\n",
    "        else:\n",
    "            number=0\n",
    "        y_test.append(number)\n",
    "\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    X_vectorized = vectorizer.transform(test_sentences_cleaned)\n",
    "    svc_pred = svc_model.predict(X_vectorized)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    svc_accuracy = accuracy_score(y_test, svc_pred)\n",
    "\n",
    "    # Evaluate F1-score\n",
    "    svc_f1_score = f1_score(y_test, svc_pred, average='weighted')\n",
    "\n",
    "    # Evaluate balanced accuracy\n",
    "    svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_pred)\n",
    "\n",
    "    # Evaluate ROC-AUC score\n",
    "    svc_roc_auc_score = roc_auc_score(y_test, svc_model.decision_function(X_vectorized), multi_class='ovr')\n",
    "\n",
    "    # Print evaluation metrics for Linear SVC on the unseen test set\n",
    "    print(\"Linear SVC Accuracy on Unseen Test Set:\", round(svc_accuracy,2))\n",
    "    print(\"Linear SVC F1-Score on Unseen Test Set:\", round(svc_f1_score,2))\n",
    "    print(\"Linear SVC Balanced Accuracy on Unseen Test Set:\", round(svc_balanced_accuracy,2))\n",
    "    print(\"Linear SVC ROC-AUC Score on Unseen Test Set:\", round(svc_roc_auc_score,2))\n",
    "    print()\n",
    "\n",
    "    # Print classification report and confusion matrix for Linear SVC on the unseen test set\n",
    "    print(\"Linear SVC Classification Report on Unseen Test Set:\")\n",
    "    print(classification_report(y_test, svc_pred))\n",
    "\n",
    "    print(\"Linear SVC Confusion Matrix on Unseen Test Set:\")\n",
    "    print(confusion_matrix(y_test, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c3e08d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9501190476190476\n",
      "Logistic Regression F1-Score: 0.9445570925210562\n",
      "Logistic Regression Balanced Accuracy: 0.773473679913486\n",
      "Logistic Regression ROC-AUC Score: 0.9648350141308188\n",
      "\n",
      "Gaussian Naive Bayes Accuracy: 0.6739880952380952\n",
      "Gaussian Naive Bayes F1-Score: 0.7391939510287234\n",
      "Gaussian Naive Bayes Balanced Accuracy: 0.70426949419908\n",
      "Gaussian Naive Bayes ROC-AUC Score: 0.70426949419908\n",
      "\n",
      "Linear SVC Accuracy: 0.9592261904761905\n",
      "Linear SVC F1-Score: 0.9572056571399592\n",
      "Linear SVC Balanced Accuracy: 0.8421455198043682\n",
      "Linear SVC ROC-AUC Score: 0.961747305508553\n",
      "\n",
      "Linear SVC Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     15129\n",
      "           1       0.87      0.70      0.77      1671\n",
      "\n",
      "    accuracy                           0.96     16800\n",
      "   macro avg       0.92      0.84      0.88     16800\n",
      "weighted avg       0.96      0.96      0.96     16800\n",
      "\n",
      "Linear SVC Confusion Matrix:\n",
      "[[14952   177]\n",
      " [  508  1163]]\n",
      "-----Now test the model on another unseen test sets--------\n",
      "Linear SVC Accuracy on Unseen Test Set: 0.92\n",
      "Linear SVC F1-Score on Unseen Test Set: 0.91\n",
      "Linear SVC Balanced Accuracy on Unseen Test Set: 0.85\n",
      "Linear SVC ROC-AUC Score on Unseen Test Set: 0.96\n",
      "\n",
      "Linear SVC Classification Report on Unseen Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95     21914\n",
      "           1       0.96      0.71      0.81      7475\n",
      "\n",
      "    accuracy                           0.92     29389\n",
      "   macro avg       0.93      0.85      0.88     29389\n",
      "weighted avg       0.92      0.92      0.91     29389\n",
      "\n",
      "Linear SVC Confusion Matrix on Unseen Test Set:\n",
      "[[21679   235]\n",
      " [ 2189  5286]]\n"
     ]
    }
   ],
   "source": [
    "text_classification(dataset,welfare_categories_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0dd013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
