{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cd81b1",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79f916d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11170]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "import subprocess as sp\n",
    "import os\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c20c53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "with open('test_dataset.json', 'r') as fp:\n",
    "    test_dataset = json.load(fp)\n",
    "with open('train_dataset.json', 'r') as fp:\n",
    "    train_dataset = json.load(fp)\n",
    "f = open('/data/data_codebook.json')\n",
    "data_codebook = json.load(f)\n",
    "super_set={}\n",
    "for s in data_codebook:\n",
    "    if s[2]!=\"domain_name\":\n",
    "        if s[2] not in super_set:\n",
    "            super_set[s[2]]=[]\n",
    "        if s[5] not in super_set[s[2]]:\n",
    "            super_set[s[2]].append(s[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1536de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    \n",
    "    html=re.compile(r'<.*?>') \n",
    "    \n",
    "    text = html.sub(r'',text) #Removing html tags\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    \n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac8ed99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dataset_counts(dataset):\n",
    "    count_dataset = {\n",
    "        \"general\": {},\n",
    "        \"detailed\": {}\n",
    "    }\n",
    "    \n",
    "    for s in dataset:\n",
    "        detailed_label = s['detailed_label']\n",
    "        general_label = s[\"general_label\"]\n",
    "        \n",
    "        if detailed_label not in count_dataset[\"detailed\"]:\n",
    "            count_dataset[\"detailed\"][detailed_label] = 0\n",
    "        count_dataset[\"detailed\"][detailed_label] += 1\n",
    "\n",
    "        if general_label not in count_dataset[\"general\"]:\n",
    "            count_dataset[\"general\"][general_label] = 0\n",
    "        count_dataset[\"general\"][general_label] += 1\n",
    "    \n",
    "    return count_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08dbbd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_dataset(balanced_dataset, total_limit, desired_percentage):\n",
    "    used_sentences = set()\n",
    "    dataset = []\n",
    "    super_category_counts = {super_category: 0 for super_category in super_set.keys()}\n",
    "    category_counts = {category: 0 for category in balanced_dataset.keys()}\n",
    "\n",
    "    # Calculate the limit for 'Welfare and Quality of Life' based on desired_percentage\n",
    "    welfare_category = 'Welfare and Quality of Life'\n",
    "    welfare_desired_limit = int(total_limit * desired_percentage)\n",
    "    # Calculate the total number of sentences in the 'Welfare and Quality of Life' super-category\n",
    "    welfare_super_category_sentences = sum(len(balanced_dataset[category]) for category in super_set[welfare_category] if category in balanced_dataset)\n",
    "    # Calculate the limits for categories within the 'Welfare and Quality of Life' super-category\n",
    "    welfare_category_limits = {}\n",
    "    for category in super_set[welfare_category]:\n",
    "        if category in balanced_dataset:\n",
    "            category_percentage = len(balanced_dataset[category]) / welfare_super_category_sentences\n",
    "            welfare_category_limits[category] = int(welfare_desired_limit * category_percentage)\n",
    "    \n",
    "    # Process 'Welfare and Quality of Life' category first\n",
    "    welfare_categories = super_set[welfare_category]\n",
    "    #print(welfare_super_category_sentences)\n",
    "    for category in welfare_categories:\n",
    "        if category not in balanced_dataset:\n",
    "            continue\n",
    "        \n",
    "        sentences = balanced_dataset[category]\n",
    "        category_limit = welfare_category_limits[category]\n",
    "        for sentence in sentences:\n",
    "            if len(dataset) >= welfare_desired_limit or len(dataset) >= total_limit:\n",
    "                break\n",
    "\n",
    "            if sentence not in used_sentences and super_category_counts[welfare_category] < welfare_desired_limit and category_limit > 0:\n",
    "                per_line_dict = {\n",
    "                    \"sentence\": sentence,\n",
    "                    \"detailed_label\": category,\n",
    "                    \"general_label\": welfare_category\n",
    "                }\n",
    "                dataset.append(per_line_dict)\n",
    "                used_sentences.add(sentence)\n",
    "                super_category_counts[welfare_category] += 1\n",
    "                category_limit -= 1    \n",
    "                \n",
    "    def get_max_value_key_and_value(dictionary):\n",
    "        max_key = max(dictionary, key=dictionary.get)\n",
    "        max_value = dictionary[max_key]\n",
    "        return max_key, max_value\n",
    "    \n",
    "    if len(dataset)<welfare_desired_limit:\n",
    "        w_max_key, w_max_value=get_max_value_key_and_value(welfare_category_limits)\n",
    "        missing_part=welfare_desired_limit-len(dataset)\n",
    "        for sentence in balanced_dataset[w_max_key]:\n",
    "            if  missing_part>0:\n",
    "                if sentence not in used_sentences and super_category_counts[welfare_category] < welfare_desired_limit:\n",
    "                    per_line_dict = {\n",
    "                        \"sentence\": sentence,\n",
    "                        \"detailed_label\": w_max_key,\n",
    "                        \"general_label\": welfare_category\n",
    "                    }\n",
    "                    dataset.append(per_line_dict)\n",
    "                    used_sentences.add(sentence)\n",
    "                    super_category_counts[welfare_category] += 1\n",
    "                    missing_part-= 1\n",
    "    # Calculate the limit for other super categories\n",
    "    other_desired_limit = total_limit - welfare_desired_limit\n",
    "    \n",
    "    def get_length(d):\n",
    "        ''' Return length of all dict values'''\n",
    "        return sum(len(v) for k, v in d.items())\n",
    "    \n",
    "    total_length=get_length(balanced_dataset)\n",
    "    non_welfare_sentences_count=total_length-welfare_super_category_sentences\n",
    "    non_welfare_category_limits = {}\n",
    "    for super_category, categories in super_set.items(): \n",
    "        if super_category==welfare_category:\n",
    "            continue\n",
    "        for category in categories:\n",
    "            if category not in balanced_dataset:\n",
    "                continue\n",
    "            non_welfare_category_percentage = len(balanced_dataset[category]) / non_welfare_sentences_count\n",
    "            non_welfare_category_limits[category] = int(other_desired_limit * non_welfare_category_percentage)\n",
    "    for super_category, categories in super_set.items(): \n",
    "        if super_category==welfare_category:\n",
    "            continue\n",
    "            \n",
    "        for category in categories:\n",
    "            if category not in balanced_dataset:\n",
    "                continue\n",
    "            sentences = balanced_dataset[category]\n",
    "            category_limit = non_welfare_category_limits[category]\n",
    "            for sentence in sentences:\n",
    "                \n",
    "                if sentence not in used_sentences and super_category_counts[super_category] < other_desired_limit and category_limit > 0:\n",
    "                    \n",
    "                    per_line_dict = {\n",
    "                        \"sentence\": sentence,\n",
    "                        \"detailed_label\": category,\n",
    "                        \"general_label\": super_category\n",
    "                    }\n",
    "                    dataset.append(per_line_dict)\n",
    "                    used_sentences.add(sentence)\n",
    "                    super_category_counts[super_category] += 1\n",
    "                    category_limit -= 1\n",
    "    \n",
    "    if len(dataset)< total_limit:\n",
    "        wn_max_key, wn_max_value=get_max_value_key_and_value(non_welfare_category_limits)\n",
    "        for super_category, categories in super_set.items(): \n",
    "            if wn_max_key in categories:\n",
    "                additional_super_category=super_category\n",
    "        missing_part=total_limit-len(dataset)\n",
    "        for sentence in balanced_dataset[wn_max_key]:\n",
    "            if  missing_part>0:\n",
    "                if sentence not in used_sentences:\n",
    "                    per_line_dict = {\n",
    "                        \"sentence\": sentence,\n",
    "                        \"detailed_label\": wn_max_key,\n",
    "                        \"general_label\": additional_super_category\n",
    "                    }\n",
    "                    dataset.append(per_line_dict)\n",
    "                    used_sentences.add(sentence)\n",
    "                    super_category_counts[additional_super_category] += 1\n",
    "                    missing_part-= 1\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d91ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in the dataset: 56000\n",
      "{'Welfare and Quality of Life': 28000, 'External Relations': 3263, 'Freedom and Democracy': 2662, 'Political System': 4201, 'Economy': 10878, 'Fabric of Society': 4772, 'Social Groups': 2224}\n",
      "56000\n",
      "97\n",
      "{'Environmental Protection': 4028, 'Culture: Positive': 3294, 'Equality: Positive': 6094, 'Welfare State Expansion': 9582, 'Welfare State Limitation': 512, 'Education Expansion': 4413, 'Education Limitation': 49, 'Private-Public Mix in Culture: Positive': 3, 'Private-Public Mix in Welfare: Positive': 21, 'Private-Public Mix in Education: Positive': 4, 'Foreign Special Relationships: Positive': 119, 'Foreign Special Relationships: Negative': 42, 'Anti-Imperialism': 19, 'Military: Positive': 672, 'Military: Negative': 161, 'Peace': 165, 'Internationalism: Positive': 1109, 'European Community/Union: Positive': 673, 'Internationalism: Negative': 80, 'European Community/Union: Negative': 185, 'Russia/USSR/CIS: Positive': 1, 'SFR Yugoslavia: Positive': 2, 'Independence: Positive': 1, 'Anti-Imperialism: Foreign Financial Influence': 34, 'Freedom and Human Rights': 575, 'Democracy': 456, 'Constitutionalism: Positive': 136, 'Constitutionalism: Negative': 108, 'Freedom': 204, 'Human Rights': 357, 'Democracy General: Positive': 698, 'Democracy General: Negative': 5, 'Representative Democracy: Positive': 28, 'Direct Democracy: Positive': 95, 'Decentralization': 977, 'Centralisation': 49, 'Governmental and Administrative Efficiency': 1859, 'Political Corruption': 408, 'Political Authority': 555, 'Communist: Negative': 2, 'Political Authority: Party Competence': 308, 'Political Authority: Personal Competence': 16, 'Political Authority: Strong government': 17, 'Transition: Pre-Democratic Elites: Negative': 8, 'Transition: Rehabilitation and Compensation': 2, 'Free Market Economy': 433, 'Incentives: Positive': 1220, 'Market Regulation': 1291, 'Economic Planning': 278, 'Corporatism/Mixed Economy': 103, 'Protectionism: Positive': 155, 'Protectionism: Negative': 100, 'Economic Goals': 734, 'Keynesian Demand Management': 97, 'Economic Growth: Positive': 907, 'Technology and Infrastructure: Positive': 3470, 'Controlled Economy': 129, 'Nationalisation': 114, 'Economic Orthodoxy': 570, 'Marxist Analysis': 51, 'Anti-Growth Economy: Positive': 420, 'Control of Economy: Negative': 5, 'Property-Restitution: Positive': 1, 'Mixed Economy: Positive': 1, 'Sustainability: Positive': 799, 'National Way of Life: Positive': 290, 'National Way of Life: Negative': 28, 'Traditional Morality: Positive': 674, 'Traditional Morality: Negative': 261, 'Law and Order: Positive': 1655, 'Civic Mindedness: Positive': 280, 'Multiculturalism: Positive': 213, 'Multiculturalism: Negative': 101, 'National Security: Positive': 7, 'Cyprus Issue': 20, 'General Crisis': 2, 'National Way of Life General: Positive': 388, 'National Way of Life: Immigration: Negative': 103, 'National Way of Life General: Negative': 47, 'National Way of Life: Immigration: Positive': 93, 'Law and Order: Negative': 81, 'Civic Mindedness General: Positive': 263, 'Civic Mindedness: Bottom-Up Activism': 26, 'Multiculturalism General: Positive': 141, 'Multiculturalism: Immigrants Diversity': 27, 'Multiculturalism: Indigenous rights: Positive': 22, 'Multiculturalism General: Negative': 32, 'Multiculturalism: Immigrants Assimilation': 17, 'Multiculturalism: Indigenous rights: Negative': 1, 'Agriculture and Farmers: Positive': 1221, 'Middle Class and Professional Groups': 213, 'Underprivileged Minority Groups': 347, 'Non-economic Demographic Groups': 424, 'Minorities Abroad: Positive': 3, 'War Participants: Positive': 6, 'Refugees: Positive': 3, 'Agriculture and Farmers: Negative': 7}\n"
     ]
    }
   ],
   "source": [
    "\"Experiment 1\"\n",
    "total_limit = 56000\n",
    "desired_percentage = 0.5\n",
    "\n",
    "dataset = create_custom_dataset(train_dataset, total_limit, desired_percentage)\n",
    "\n",
    "print(\"Total sentences in the dataset:\", len(dataset))\n",
    "\n",
    "# Usage example:\n",
    "# Assuming you have the 'dataset' variable containing the dataset obtained from the create_custom_dataset function\n",
    "# Replace this with the actual dataset you want to count.\n",
    "count_dataset = calculate_dataset_counts(dataset)\n",
    "print(count_dataset[\"general\"])\n",
    "print(sum(count_dataset[\"general\"].values()))\n",
    "print(len(count_dataset[\"detailed\"].keys()))\n",
    "print(count_dataset[\"detailed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe0125",
   "metadata": {},
   "source": [
    "# RoBerta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49afe225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokens\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "226eee6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>detailed_label</th>\n",
       "      <th>general_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5750</th>\n",
       "      <td>The Party for a European Future (PEI) will ad...</td>\n",
       "      <td>Culture: Positive</td>\n",
       "      <td>Welfare and Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24333</th>\n",
       "      <td>Management of the school calendar;</td>\n",
       "      <td>Education Expansion</td>\n",
       "      <td>Welfare and Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41335</th>\n",
       "      <td>Qualifications of the existing housing stock ...</td>\n",
       "      <td>Economic Planning</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47741</th>\n",
       "      <td>The ecological footprint is a new method for ...</td>\n",
       "      <td>Anti-Growth Economy: Positive</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48172</th>\n",
       "      <td>In any case, more clarity must be provided wi...</td>\n",
       "      <td>Sustainability: Positive</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26025</th>\n",
       "      <td>Cooperation on innovation and science</td>\n",
       "      <td>Education Expansion</td>\n",
       "      <td>Welfare and Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26323</th>\n",
       "      <td>We will support its openness and integration ...</td>\n",
       "      <td>Education Expansion</td>\n",
       "      <td>Welfare and Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55154</th>\n",
       "      <td>The co-payment is indeed more important for t...</td>\n",
       "      <td>Middle Class and Professional Groups</td>\n",
       "      <td>Social Groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49570</th>\n",
       "      <td>Mothers take care of children at home signifi...</td>\n",
       "      <td>Traditional Morality: Positive</td>\n",
       "      <td>Fabric of Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>Last but not least, the question of final dis...</td>\n",
       "      <td>Environmental Protection</td>\n",
       "      <td>Welfare and Quality of Life</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "5750    The Party for a European Future (PEI) will ad...   \n",
       "24333                 Management of the school calendar;   \n",
       "41335   Qualifications of the existing housing stock ...   \n",
       "47741   The ecological footprint is a new method for ...   \n",
       "48172   In any case, more clarity must be provided wi...   \n",
       "...                                                  ...   \n",
       "26025              Cooperation on innovation and science   \n",
       "26323   We will support its openness and integration ...   \n",
       "55154   The co-payment is indeed more important for t...   \n",
       "49570   Mothers take care of children at home signifi...   \n",
       "2248    Last but not least, the question of final dis...   \n",
       "\n",
       "                             detailed_label                general_label  \n",
       "5750                      Culture: Positive  Welfare and Quality of Life  \n",
       "24333                   Education Expansion  Welfare and Quality of Life  \n",
       "41335                     Economic Planning                      Economy  \n",
       "47741         Anti-Growth Economy: Positive                      Economy  \n",
       "48172              Sustainability: Positive                      Economy  \n",
       "...                                     ...                          ...  \n",
       "26025                   Education Expansion  Welfare and Quality of Life  \n",
       "26323                   Education Expansion  Welfare and Quality of Life  \n",
       "55154  Middle Class and Professional Groups                Social Groups  \n",
       "49570        Traditional Morality: Positive            Fabric of Society  \n",
       "2248               Environmental Protection  Welfare and Quality of Life  \n",
       "\n",
       "[56000 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(data=(dataset))\n",
    "dataframe=shuffle(dataframe).dropna()\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b6ec85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences=list(dataframe[\"sentence\"])\n",
    "train_labels=[]\n",
    "for s in list(dataframe[\"general_label\"]):\n",
    "    if s=='Welfare and Quality of Life':\n",
    "        number=1\n",
    "    else:\n",
    "        number=0\n",
    "    train_labels.append(number)\n",
    "\n",
    "train_sentences_cleaned=[]\n",
    "for s in list(dataframe[\"sentence\"]):\n",
    "    cleaned=clean_text(s)\n",
    "    train_sentences_cleaned.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62bdc231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:  135\n",
      "min:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'The distribution of sequence length, when the percentage of Welfare sentences is: 50.0%')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the text feature \n",
    "tokenized_feature_raw = tokenizer.batch_encode_plus(\n",
    "                            # Sentences to encode\n",
    "                            train_sentences_cleaned, \n",
    "                            # Add '[CLS]' and '[SEP]'\n",
    "                            add_special_tokens = True      \n",
    "                   )\n",
    "# collect tokenized sentence length \n",
    "token_sentence_length = [len(x) for x in tokenized_feature_raw['input_ids']]\n",
    "print('max: ', max(token_sentence_length))\n",
    "print('min: ', min(token_sentence_length))\n",
    "# plot the distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.hist(token_sentence_length, rwidth = 0.9)\n",
    "plt.xlabel('Sequence Length', fontsize = 18)\n",
    "plt.ylabel('Number of Samples', fontsize = 18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.title(\"The distribution of sequence length, when the percentage of Welfare sentences is: \"+str(desired_percentage*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49e63e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize features \n",
    "MAX_LEN = 135\n",
    "tokenized_feature = tokenizer.batch_encode_plus(\n",
    "                            # Sentences to encode\n",
    "                            train_sentences_cleaned, \n",
    "                            # Add '[CLS]' and '[SEP]'\n",
    "                            add_special_tokens = True,\n",
    "                            # Add empty tokens if len(text)<MAX_LEN\n",
    "                            padding = 'max_length',\n",
    "                            # Truncate all sentences to max length\n",
    "                            truncation=True,\n",
    "                            # Set the maximum length\n",
    "                            max_length = MAX_LEN, \n",
    "                            # Return attention mask\n",
    "                            return_attention_mask = True,\n",
    "                            # Return pytorch tensors\n",
    "                            return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "361641ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 80% for training and 20% for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(tokenized_feature['input_ids'], \n",
    "                                                                                                             train_labels,\n",
    "                                                                                                                    tokenized_feature['attention_mask'],\n",
    "                                                                                                      random_state=42, test_size=0.2, stratify=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "329cf7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base package for the tasks from pytorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# define batch_size\n",
    "batch_size = 16\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, torch.tensor(train_labels))\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# Create the DataLoader for our test set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, torch.tensor(validation_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3e573e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base package for the tasks from pytorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d468d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BertForSequenceClassification\n",
    "from transformers import XLMRobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\", \n",
    "    # Specify number of classes\n",
    "    num_labels = len(set(train_labels)), \n",
    "    # Whether the model returns attentions weights\n",
    "    output_attentions = False,\n",
    "    # Whether the model returns all hidden-states \n",
    "    output_hidden_states = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73091f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 250002. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(250002, 768, padding_idx=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Receive the full size of the new word\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a73d782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/yabdul/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimizer & Learning Rate Scheduler\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a3043c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 3\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f366a34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tell pytorch to run this model on GPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23de45ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m device\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ebc3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "import time\n",
    "import datetime\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "torch.cuda.empty_cache()\n",
    "# start training from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c83f4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.53\n",
      "  Training epcoh took: 0:12:24\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.78\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.46\n",
      "  Training epcoh took: 0:12:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.79\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epcoh took: 0:12:24\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.79\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:40:49 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the device using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)        \n",
    "        loss = output.loss\n",
    "        total_train_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    best_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "            output= model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "        loss = output.loss\n",
    "        total_eval_loss += loss.item()\n",
    "        # Move logits and labels to CPU if we are using GPU\n",
    "        logits = output.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    if avg_val_accuracy > best_eval_accuracy:\n",
    "        torch.save(model, 'Roberta_50percent_TwoStep_General')\n",
    "        best_eval_accuracy = avg_val_accuracy\n",
    "    #print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    #print(\"  Validation took: {:}\".format(validation_time))\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "063c2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('Roberta_50percent_TwoStep_General')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c6b8b",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0bae313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Now test the model on another unseen test sets--------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>detailed_label</th>\n",
       "      <th>general_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32320</th>\n",
       "      <td>1. Clear division of functions, competences a...</td>\n",
       "      <td>Culture: Positive</td>\n",
       "      <td>Welfare and Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67177</th>\n",
       "      <td>Because of this, there is a great pressure of...</td>\n",
       "      <td>Economic Growth: Positive</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13601</th>\n",
       "      <td>We believe that tax policy can and should be u...</td>\n",
       "      <td>Incentives: Positive</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83142</th>\n",
       "      <td>We reject capitalism, which is in line with t...</td>\n",
       "      <td>Internationalism: Negative</td>\n",
       "      <td>External Relations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17192</th>\n",
       "      <td>while incorporating clear conditionality for ...</td>\n",
       "      <td>Economic Orthodoxy</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69315</th>\n",
       "      <td>While our country has risen to the top catego...</td>\n",
       "      <td>Democracy General: Positive</td>\n",
       "      <td>Freedom and Democracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8931</th>\n",
       "      <td>The Center Party will secure and develop this...</td>\n",
       "      <td>Nationalisation</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35872</th>\n",
       "      <td>Only a corrupt, moral, law-abiding public spi...</td>\n",
       "      <td>Law and Order: Positive</td>\n",
       "      <td>Fabric of Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62620</th>\n",
       "      <td>We enacted the Banking Law in order to ensure...</td>\n",
       "      <td>Political Corruption</td>\n",
       "      <td>Political System</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51892</th>\n",
       "      <td>C. RENEWABLE ENERGY SOURCES</td>\n",
       "      <td>Environmental Protection</td>\n",
       "      <td>Welfare and Quality of Life</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84860 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "32320   1. Clear division of functions, competences a...   \n",
       "67177   Because of this, there is a great pressure of...   \n",
       "13601  We believe that tax policy can and should be u...   \n",
       "83142   We reject capitalism, which is in line with t...   \n",
       "17192   while incorporating clear conditionality for ...   \n",
       "...                                                  ...   \n",
       "69315   While our country has risen to the top catego...   \n",
       "8931    The Center Party will secure and develop this...   \n",
       "35872   Only a corrupt, moral, law-abiding public spi...   \n",
       "62620   We enacted the Banking Law in order to ensure...   \n",
       "51892                        C. RENEWABLE ENERGY SOURCES   \n",
       "\n",
       "                    detailed_label                general_label  \n",
       "32320            Culture: Positive  Welfare and Quality of Life  \n",
       "67177    Economic Growth: Positive                      Economy  \n",
       "13601         Incentives: Positive                      Economy  \n",
       "83142   Internationalism: Negative           External Relations  \n",
       "17192           Economic Orthodoxy                      Economy  \n",
       "...                            ...                          ...  \n",
       "69315  Democracy General: Positive        Freedom and Democracy  \n",
       "8931               Nationalisation                      Economy  \n",
       "35872      Law and Order: Positive            Fabric of Society  \n",
       "62620         Political Corruption             Political System  \n",
       "51892     Environmental Protection  Welfare and Quality of Life  \n",
       "\n",
       "[84860 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"-----Now test the model on another unseen test sets--------\")\n",
    "test_dataframe=[]\n",
    "for k,v in test_dataset.items():\n",
    "    for key, value in super_set.items():\n",
    "        if k in value:\n",
    "            super_label = key\n",
    "    for s in v:\n",
    "            per_line_dict = {}\n",
    "            per_line_dict[\"sentence\"] = s\n",
    "            per_line_dict[\"detailed_label\"] = k\n",
    "            per_line_dict[\"general_label\"] = super_label\n",
    "            test_dataframe.append(per_line_dict)\n",
    "\n",
    "test_dataframe = pd.DataFrame(data=(test_dataframe))\n",
    "test_dataframe=shuffle(test_dataframe).dropna()\n",
    "test_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6aea2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences_cleaned=[]\n",
    "for s in list(test_dataframe[\"sentence\"]):\n",
    "    cleaned=clean_text(s)\n",
    "    test_sentences_cleaned.append(cleaned)\n",
    "\n",
    "test_labels_numbers=[]\n",
    "for s in list(test_dataframe[\"general_label\"]):\n",
    "    if s=='Welfare and Quality of Life':\n",
    "        number=1\n",
    "    else:\n",
    "        number=0\n",
    "    test_labels_numbers.append(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c14491a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sentences_cleaned)==len(test_labels_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ec1a2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:  151\n",
      "min:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test set.The distribution of sequence length, when the percentage of Welfare sentences is: 50.0%')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the text feature \n",
    "test_tokenized_feature = tokenizer.batch_encode_plus(\n",
    "                            # Sentences to encode\n",
    "                            test_sentences_cleaned, \n",
    "                            # Add '[CLS]' and '[SEP]'\n",
    "                            add_special_tokens = True      \n",
    "                   )\n",
    "# collect tokenized sentence length \n",
    "token_sentence_length = [len(x) for x in test_tokenized_feature['input_ids']]\n",
    "print('max: ', max(token_sentence_length))\n",
    "print('min: ', min(token_sentence_length))\n",
    "# plot the distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.hist(token_sentence_length, rwidth = 0.9)\n",
    "plt.xlabel('Sequence Length', fontsize = 18)\n",
    "plt.ylabel('Number of Samples', fontsize = 18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.title(\"Test set.The distribution of sequence length, when the percentage of Welfare sentences is: \"+str(desired_percentage*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dea8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize features \n",
    "MAX_LEN = 151\n",
    "test_tokenized_feature = tokenizer.batch_encode_plus(\n",
    "                            # Sentences to encode\n",
    "                            test_sentences_cleaned, \n",
    "                            # Add '[CLS]' and '[SEP]'\n",
    "                            add_special_tokens = True,\n",
    "                            # Add empty tokens if len(text)<MAX_LEN\n",
    "                            padding = 'max_length',\n",
    "                            # Truncate all sentences to max length\n",
    "                            truncation=True,\n",
    "                            # Set the maximum length\n",
    "                            max_length = MAX_LEN, \n",
    "                            # Return attention mask\n",
    "                            return_attention_mask = True,\n",
    "                            # Return pytorch tensors\n",
    "                            return_tensors = 'pt'     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09c8d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for our test set\n",
    "validation_data = TensorDataset(test_tokenized_feature['input_ids'], test_tokenized_feature['attention_mask'], torch.tensor(test_labels_numbers))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "245af2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used is: 467.57 s\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import numpy as np\n",
    "t0 = time.time()\n",
    "# model in validation mode\n",
    "model.eval()\n",
    "# save prediction\n",
    "predictions,true_labels =[],[]\n",
    "# evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask)\n",
    "    # get output\n",
    "    logits = outputs[0]\n",
    "    # move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    final_prediction = np.argmax(logits, axis=-1).flatten()\n",
    "    predictions.append(final_prediction)\n",
    "    true_labels.append(label_ids)\n",
    "    \n",
    "print('total time used is: {0:.2f} s'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9adc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51f50af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Accuracy on Unseen Test Set: 0.78\n",
      "Linear SVC F1-Score on Unseen Test Set: 0.79\n",
      "Linear SVC Balanced Accuracy on Unseen Test Set: 0.78\n",
      "\n",
      "Linear SVC Classification Report on Unseen Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.78      0.83     55471\n",
      "           1       0.66      0.79      0.72     29389\n",
      "\n",
      "    accuracy                           0.78     84860\n",
      "   macro avg       0.77      0.78      0.77     84860\n",
      "weighted avg       0.80      0.78      0.79     84860\n",
      "\n",
      "Linear SVC Confusion Matrix on Unseen Test Set:\n",
      "[[43378 12093]\n",
      " [ 6241 23148]]\n"
     ]
    }
   ],
   "source": [
    "# convert numeric label to string\n",
    "final_prediction_list = np.concatenate(predictions)\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(test_labels_numbers, final_prediction_list)\n",
    "\n",
    "# Evaluate F1-score\n",
    "f1_score = f1_score(test_labels_numbers, final_prediction_list, average='weighted')\n",
    "\n",
    "# Evaluate balanced accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(test_labels_numbers, final_prediction_list)\n",
    "\n",
    "\n",
    "# Print evaluation metrics for Linear SVC on the unseen test set\n",
    "print(\"Linear SVC Accuracy on Unseen Test Set:\", round(accuracy,2))\n",
    "print(\"Linear SVC F1-Score on Unseen Test Set:\", round(f1_score,2))\n",
    "print(\"Linear SVC Balanced Accuracy on Unseen Test Set:\", round(balanced_accuracy,2))\n",
    "print()\n",
    "\n",
    "# Print classification report and confusion matrix for Linear SVC on the unseen test set\n",
    "print(\"Linear SVC Classification Report on Unseen Test Set:\")\n",
    "print(classification_report(test_labels_numbers, final_prediction_list))\n",
    "\n",
    "print(\"Linear SVC Confusion Matrix on Unseen Test Set:\")\n",
    "print(confusion_matrix(test_labels_numbers, final_prediction_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d280e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f209b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
